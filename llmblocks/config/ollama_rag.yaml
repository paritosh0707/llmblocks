rag:
  name: "ollama_rag"
  description: "RAG pipeline using Ollama local models"
  
  # Document processing
  chunk_size: 1000
  chunk_overlap: 200
  text_splitter_type: "recursive"
  
  # Vector store configuration
  vector_store_type: "chroma"
  embedding_model: "text-embedding-ada-002"
  persist_directory: "./data/vectorstore"
  
  # LLM configuration
  llm_provider: "ollama"
  llm_model: "llama2"
  llm_base_url: "http://localhost:11434"  # Ollama server URL
  temperature: 0.1
  max_tokens: 1000
  
  # Retrieval configuration
  top_k: 4
  similarity_threshold: 0.7
  
  # Memory configuration
  memory_enabled: true
  memory_type: "in_memory"
  
  # Streaming configuration
  streaming_enabled: false
  
  # Additional metadata
  metadata:
    version: "1.0.0"
    environment: "local"
    tags: ["rag", "ollama", "local", "llama2"] 